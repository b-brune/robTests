---
title: "Getting started with `robTests`"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
csl: csda.csl
vignette: >
  %\VignetteIndexEntry{introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction {#introduction}
The package `robTests` contains robust (approximately) distribution-free tests for the two-sample location problem.

We consider the following data situation: Let $\boldsymbol{X} = \left(X_1, \ldots, X_m\right)$ 
and $\boldsymbol{Y} = \left(Y_1, \ldots, Y_n\right)$ be two samples of sizes $m, n \in \mathbb{N}$.
Each sample consists of independent and identically distributed (i.i.d.) random variables so that
\begin{align*}
  X_1, \ldots, X_m \overset{i.i.d.}{\sim} F \quad \text{and} \quad Y_1, \ldots, Y_n \overset{i.i.d.}{\sim} G,
\end{align*}
where $F, G\colon\ \mathbb{R} \to \left[0, 1\right]$ are the cumulative distribution functions
of the underlying unknown continuous distributions. Let $G$ be a shifted version of $F$, i.e.
\begin{align*}
  G\left(x\right) = F\left(x + \Delta\right) \quad \text{for all}\ x \in \mathbb{R},
\end{align*}
with $\Delta \in \mathbb{R}$ denoting the shift size. Hence, $F$ and $G$ differ at
most in location and, in particular, have the same variance $\sigma^2 := \sigma_X^2 = \sigma_Y^2 > 0$ (homoscedasticity).

The following hypotheses can be tested:
\begin{align*}
  &H_0^{(=)}\colon\ \Delta = \Delta_0 \quad \text{vs.} \quad H_1^{(=)}\colon\ \Delta \neq \Delta_0 \\
  &H_0^{(\leq)}\colon\ \Delta \leq \Delta_0 \quad \text{vs.} \quad H_1^{(\leq)}\colon\ \Delta > \Delta_0 \\
  &H_0^{(\geq)}\colon\ \Delta \geq \Delta_0 \quad \text{vs.} \quad H_1^{(\geq)}\colon\ \Delta < \Delta_0,
\end{align*}
where $\Delta_0 \in \mathbb{R}$ is a value to which $\Delta$ is set into relation.

A popular test for this scenario is the ordinary two-sample $t$-test, which uses the normality assumption. For large samples, the central limit theorem ensures that the test keeps the desired significance level $\alpha \in \left(0, 1\right)$, 
even if the underlying distributions are not normal. However, relying on the central 
limit theorem is often not appropriate in small samples. Moreover, the test is known 
to be vulnerable to outlying values as they can mask existing location shifts or lead to wrong rejections of
the null hypothesis. 

The Wilcoxon-Mann-Whitney test is a popular distribution-free test which is nearly
as powerful as the $t$-test under normality but can be more powerful under non-normal
distributions. It is often preferred over the $t$-test, when the normality assumption
cannot be justified, but can be nearly as vulnerable to outliers [@FriGat07rank].

We implemented tests for the outlined situation with the following objectives in mind:

* robustness against outliers
* (approximately) distribution free, i.e. keep the significance level under $H_0$ over all continuous distributions
* have a large power over several distributions.

In this vignette, we describe the basic functionality of the package and the
implemented tests.

In addition to the two-sample location problem, the tests can also be used to
detect scale differences between two independent samples. This will be briefly
discussed at the end of the vignette.

In the remainder of this vignette, let $\boldsymbol{x} = \left(x_1, \ldots, x_m\right)$ and $\boldsymbol{y} = \left(y_1, \ldots, y_n\right)$ be observed samples from both distributions.

```{r}
library(robTests)

sessionInfo()
```

# Overview on implemented tests
The package contains the following two-sample tests:
  
Function name | Test names | Description | Literature
------------- | --------- | ----------- | ----------
`med_test`    | MED1-test, MED2-test | Tests based on the difference of the sample medians | [@FriDeh11robu]
`hl1_test`    | HL11-test, HL12-test | Tests based on the one-sample Hodges-Lehmann estimator | [@FriDeh11robu]
`hl2_test`    | HL21-test, HL22-test | Tests based on the two-sample Hodges-Lehmann estimator | [@FriDeh11robu]

We describe the test statistics in detail in the next subsection. All functions
contain several arguments to adjust the functions to certain data situations and
objectives.

## Test statistics
We use test statistics which follow the construction principle of the $t$-statistic, i.e.
an estimator $\hat{\Delta}$ for the true location difference $\Delta$ between both samples is divided
by a pooled estimator $\hat{S}$ for the unknown standard deviation $\sigma$, leading to test
statistics of the form
\begin{align*}
  T = \frac{\hat{\Delta} - \Delta_0}{\hat{S}}.
\end{align*}
In the $t$-statistic, $\hat{\Delta}$ is estimated by $\overline{x} - \overline{y}$,
where $\overline{x}$ and $\overline{y}$ are the sample means of $\boldsymbol{x}$ and
$\boldsymbol{y}$, respectively. The denominator $\hat{S}$ is the pooled empirical 
standard deviation. We replace both estimators by the robust estimators described 
in the following subsections.

The argument `alternative` in the function of an implemented test determines for
which hypothesis pair the test is performed. The following table shows the different
options for the simplified case $\Delta_0 = 0$.

Value of argument `alternative` | Alternative hypothesis | Meaning of the alternative hypothesis
--------------------- | ------------ | -----------
`two.sided`                     | $H_1^{(=)}\colon\ \Delta \neq 0$ | $X$ and $Y$ are stocastically unequal.
`greater`                       | $H_1^{(\leq)}\colon\ \Delta > 0$ | $X$ is stochastically larger than $Y$.
`less`                          | $H_1^{(\geq)}\colon\ \Delta < 0$ | $X$ is stochastically smaller than $Y$.

### MED-tests
An obvious possibility to obtain a robust estimator for $\Delta$ is to 
replace the sample means by the sample medians, i.e. the location difference
is estimated by

\begin{align*}
   \hat{\Delta}^{(\text{MED})} = \text{median}\left(x_1, \ldots, x_m\right) - \text{median}\left(y_1, \ldots, y_n\right).
\end{align*}

The test can be used with the function `med_test`.

Two different options for estimating the within-sample variability are available. 
Setting the argument `scale = S3`, the scale is estimated by the median of the 
absolute deviations of each observation from its corresponding sample median:

\begin{align*}
  \hat{S} = 2 \cdot \text{median}\left\{|x_1 - \tilde{x}|, \ldots, |x_m - \tilde{x}|, |y_1 - \tilde{y}|, \ldots, |y_n - \tilde{y}|\right\},
\end{align*}
where $\tilde{x}$ and $\tilde{y}$ are the sample medians.

Another possibility is to set `scale = S4` to estimate the scale by the sum of the median absolute deviations
from the sample median (MAD) of each sample:

\begin{align*}
  \hat{S} = 1.4826 \cdot \text{median}\left\{|x_1 - \tilde{x}|, \ldots, |x_m - \tilde{x}|\right\} + 1.4826 \cdot \text{median}\left\{|y_1 - \tilde{y}|, \ldots, |y_n - \tilde{y}|\right\}.
\end{align*}

The constant `1.4826` allows for an unbiased estimation of the sample standard
deviation by the MAD.

### HL1-tests
A known disadvantage of the sample median is its low efficiency under normality
compared to the sample mean. Estimators that provide a compromise between robustness
and efficiency are the Hodges-Lehmann estimators [@HodLeh63esti].

An estimator for the location difference based on the one-sample Hodges-Lehmann
estimator (HL1-estimator) is given by

\begin{align*}
 \hat{\Delta}^{(\text{HL1})} = \text{median}\left\{\frac{x_i + x_j}{2}\colon\ 1 \leq i < j \leq m\right\} - \text{median}\left\{\frac{y_i + y_j}{2}\colon\ 1 \leq i < j \leq n\right\}.
\end{align*}

The test is performed by the function `hl1_test`.

By setting `scale = S1`, the within-sample variability is estimated by the
median of the absolute pairwise differences within both samples:
\begin{align*}
  \text{median}\left\{|x_i - x_j|\colon\ 1 \leq i < j \leq m,\ |y_i - y_j|\colon\ 1 \leq i < j \leq n\right\}.
\end{align*}

Using `scale = S2` estimates it by the absolute pairwise differences within the
joint sample, where every observation is centred by its corresponding sample median:
\begin{align*}
  \text{median}\left\{|z_i - z_j|\colon\ 1 \leq i < j \leq m + n\right\},
\end{align*}
where
\begin{align*}
  \left(z_1, \ldots, z_{m + n}\right) = \left(x_1 - \tilde{x}, \ldots, x_m - \tilde{x}, y_1 - \tilde{y}, \ldots, y_n - \tilde{y}\right).
\end{align*}

### HL2-tests
Instead of estimating the location difference by subtracting the estimated
locations of both samples, the two-sample Hodges-Lehmann estimator (HL2-estimator)
estimates the location difference directly:

\begin{align*}
   \hat{\Delta}^{(\text{HL2})} = \text{median}\left\{|x_i - y_j|\colon\ 1 \leq i \leq m,\ 1 \leq j \leq n\right\}.
\end{align*}

It can be performed by using the function `hl2_test`. The scale estimators are the
same as for the HL1 test.

## Computation of p-values
We implemented several ways to compute $p$-values for the different tests, which
are briefly described in this section.

The functions for each test have the argument `method`, which can be set to
`method = "permutation"` for a permutation test, `method = "randomization"` for a
randomization test, and `method = "asymptotic"` for an asymptotic test.

### Permutation test
With the permutation principle, the $p$-value of the test is obtained by
computing the value of the test statistic for all possible splits of
the joint sample $\left(x_1, \ldots, x_m, y_1, \ldots, y_n\right)$ into two
samples of sizes $m$ and $n$. This is achieved by permuting the joint sample and
splitting the permutations so that the first $m$ observations form the first
and the last $n$ observations form the second sample. This leads to $B = \binom{m}{m + n}$
possible splits. The idea is that under the null hypothesis, each of these
permutations is equally likely to be generated from the data generating distribution.

Using the permutation principle leads to distribution-free tests, i.e., a permutation
test holds its significance level under every continuous distribution.

Let $A$ be the number of splits leading to values which are at least as extreme as
the observed value of the test statistic. More formally, let $t_i$ be the
value of the test statistic for split $i$, $i = 1, \ldots, B$, and $t^{(\text{obs})}$
the observed value of the test statistic. Then,

- for $H_0^{(=)}$: $A = \#\left(|t_i| \geq |t^{(\text{obs})|}\right)$
- for $H_0^{(\leq)}$: $A = \#\left(t_i \geq t^{(\text{obs})}\right)$
- for $H_0^{(\geq)}$: $A = \#\left(t_i \leq t^{(\text{obs})}\right)$

and $p$-value $= \frac{A}{B}$.

As the number of splits to enumerate over increases rapidly in $m$ and $n$,
using the permutation principle can lead to memory and computation time issues.
Therefore, we recommend this approach only for very small sample sizes or when
sufficient computational resources are available.

### Randomization
The randomization principle is a commonly used way to deal with the aforementioned
shortcomings of the permutation principle. Instead of computing all possible
splits, only a random subset of $b << B$ splits is selected. As commonly proposed
in the literature,we draw these random subsets with replacement, i.e., a single
permutation may be drawn repeatedly.

The $p$-value can then be estimated by $\frac{A + 1}{b + 1}$, where $A$ now
relates to the drawn subsets of the splits. In the numerator and the denominator,
the number $1$ is added to include the observed value of the test statistic.

This estimator will overestimate the true $p$-value the observed sample can be
included multiple times in the drawn splits. Following [@PhiSmy10perm],
we correct the $p$-value using the function \code{permp} from the R package
\code{statmod}.

### Asymptotic
For very large sample sizes, asymptotical $p$-values can be computed for each
test. This leads to an additional drop in the computation time as compared to the
randomization principle.

Using the asymptotic distribution of the sample median, an asymptotic test statistic
for the MED-test is
\begin{align*}
  T_a^{(\text{MED})} = \sqrt{\frac{m \cdot n}{m + n}} \cdot 2 \cdot f\left(F^{-1}\left(0.5\right)\right) \cdot  \hat{\Delta}^{(\text{MED})} \overset{\text{asympt.}}{\sim} \mathcal{N}\left(0, 1\right),
\end{align*}
where $f$ is the density function belonging to the cumulative distribution function $F$. The value $f\left(F^{-1}\left(0.5\right)\right)$ can be estimated by a kernel-density estimator on the
sample $x_1 - \tilde{x}, \ldots, x_m - \tilde{x}, y_1 - \tilde{y}, \ldots, y_n - \tilde{y}$.

For the HL2-test and $\lambda = \frac{m}{n + n} \in \left(0, 1\right)$, we use the asymptotic test statistic
\begin{align*}
T_a^{(\text{HL2})} = \sqrt{12 \cdot \lambda \cdot \left(1 - \lambda\right)} \int_{-\infty}^{\infty} f^2\left(x\right) \mathrm{d}x \cdot \left(m + n\right) \cdot \hat{\Delta}^{(\text{HL2})} \overset{\text{asympt.}}{\sim} \mathcal{N}\left(0, 1\right),
\end{align*}
where $\int_{-\infty}^{\infty} f^2\left(x\right) \mathrm{d}x$ is the value of the density of $X - Y$ at zero and
can be estimated by a kernel-density estimator on the within-sample pairwise differences
$x_2 - x_1, \ldots, x_m - x_1, \ldots, x_m - x_{m - 1}, y_2 - y_1, \ldots, y_n - y_1, \ldots, y_n - y_{n - 1}$.

To obtain an asymptotic HL1-test, we replace $\hat{\Delta}^{(\text{HL2})}$ by
$\hat{\Delta}^{(\text{HL1})}$. For the kernel-density estimation, we use the R function
\code{densitiy} from the \code{stats} package.

# Testing for a location difference
We will now describe several possibilities to test for a location difference with
the functions implemented in the package.

## Continuous data
- We start with data from a continuous distribution

```{r}
set.seed(108)
x <- rnorm(10)
y <- rnorm(10)
```


### Permutation test
- Example HL1-test with scale estimator $\hat{S}^{(2)}$
- $\Delta_0 = 0$

```{r, cache = TRUE}
## Two-sided permutation test assuming no location difference between the samples under the null hypothesis
hl1_test(x = x, y = y, alternative = "two.sided", delta = 0, method = "permutation", scale = "S2")
```

- Output is object of class `htest`.
- p-value can be accessed by `$p.value`
- test statistic can be accessed by `$statistic`
- individual HL1 estimates can be asscess by `$estimate`

### Randomization test
- draw 10000 splits randomly with replacement

```{r, cache = TRUE}
## Two-sided randomization test assuming no location difference between the samples under the null hypothesis
set.seed(47)
hl1_test(x = x, y = y, alternative = "two.sided", delta = 0, method = "randomization", scale = "S2", n.rep = 10000)
```

- $p$-value close to the one computed from permutation test

## Asymptotic test
- no scale estimator necessary

```{r}
  ## Asymptotic two-sided test assuming no location difference between the samples under the null hypothesis
  hl1_test(x = x, y = y, alternative = "two.sided", delta = 0, method = "asymptotic")
```

  We can also perform the test when assuming that a certain location difference between both samples exists under $H_0$, i.e. $\Delta_0 \neq 0$.

  ```{r}
  x1 <- x + 5
  ## Asymptotic two-sided test assuming \Delta_0 = 5
  hl1_test(x = x1, y = y, alternative = "two.sided", delta = 5, method = "asymptotic")
  ```

## Discrete data
In many applications the data may be rounded to a small number of digits or the data-generating process may be discrete.

- scale estimator may be zero (no location difference) -> test statistic cannot be computed
- reduced power of the test, i.e. location differences may not be found/can vanish due to rounding


We implemented a procedure call wobbling [@FriGat07rank], i.e. we add random noise from a uniform distribution to the
observations. To keep the alteration of the original observations as small as possible, the scale of the noise is chosen
depending on the number of decimal places.

The addition of random noise is controlled by the argument `wobble`, where `wobble = TRUE` means that noise is added and
`wobble = FALSE` means that no noise is added.

```{r}
## No location difference
## Generate random sample
#set.seed(356)
# set.seed(423)
x <- rnorm(10, mean = 2)
y <- rnorm(10)

## Value of the scale estimator S1
#set.seed(3274)
set.seed(47)
hl1_test(x, y, alternative = "two.sided", method = "randomization", scale = "S1")
# p-value smaller than 0.05

#set.seed(3274)
set.seed(47)
hl1_test(round(x), round(y), alternative = "two.sided", method = "randomization", scale = "S1")
# p-value much larger than 0.05

#set.seed(3274)
set.seed(47)
hl1_test(round(x), round(y), alternative = "two.sided", method = "randomization", scale = "S1", wobble = TRUE)
```


- Wobbling (auch bei asymptotischen Tests?)

# Testing for a difference in scale
Following [@Fri12onli] the argument `var.test` allows to decide whether the
samples should be tested for a location difference (`var.test = FALSE`), which is
the default, or for different variances (`var.test = TRUE`), i.e. $\sigma^2_X \neq \sigma^2_Y$.

Setting `var.test = TRUE` transforms the observations so that a possible variance
difference between the samples appears as a location difference between the
transformed samples.

The resulting variance tests can resist outliers and heavy tailes, and work
well under asymmetry.

- Vergleich mit Standardtests ansprechen.

## Idea
Again, we start with two samples $X_1, \ldots, X_m$ and $Y_1, \ldots, Y_n$, each
of which consists of i.i.d. random variables. However, now we assume that we can write the random variables as
\begin{align*}
  X_i = \sigma_X \cdot \varepsilon_{X, i},\ $i = 1, \ldots, m$ \quad \text{and} \quad Y_j = \sigma_Y \cdot \varepsilon_{Y, j},\ $j = 1, \ldots, n$.
\end{align*}
Here, $\sigma_X$ is the standard deviation of the first sample and $\sigma_Y$ is the standard deviation of the second sample. The random variables $\varepsilon_{X, 1}, \ldots, \varepsilon_{X, m}$ and $\varepsilon_{Y, 1}, \ldots, \varepsilon_{Y, m}$ are i.i.d. random variables with expectation zero and unit variance.

We address the following hypotheses pairs:
\begin{align*}
  &H_0^{(=)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} = 1 \quad \text{vs.} \quad H_1^{(\neq)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} \neq 1 \\
  &H_0^{(>)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} \leq 1 \quad \text{vs.} \quad H_1^{(>)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} > 1 \\
  &H_0^{(<)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} \geq 1 \quad \text{vs.} \quad H_1^{(<)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} < 1.
\end{align*}

By log-transforming the random variables, we obtain
\begin{align*}
  U_i = \log\left(X_i^2\right) = \log\left(\sigma_X^2\right) + \log\left(\varepsilon_{X, i}^2\right),\ i = 1, \ldots, m, \quad \text{and} \quad V_i = \log\left(Y_j^2\right) = \log\left(\sigma_Y^2\right) + \log\left(\varepsilon_{Y, j}^2\right), j = 1, \ldots, n.
\end{align*}
This leads to $\text{E}\left(U_i\right) = \log\left(\sigma_X^2\right),\ i = 1, \ldots, m$ and $\text{E}\left(V_i\right) = \log\left(\sigma_Y^2\right),\ j = 1, \ldots, n$. Under the previously made assumptions, this means that we are in the same situation as in the Introduction. Therefore, unequal standard deviations $\sigma_X$ and $\sigma_Y$ lead to a location difference of size $\log\left(\sigma_X^2\right) - \log\left(\sigma_Y^2\right)$ in the transformed samples.

```{r, cache = TRUE}
set.seed(108)

x <- rnorm(30)
y <- rnorm(30)

## Asymptotic two-sided test assuming no scale difference between both samples
hl1_test(x = x, y = y, alternative = "two.sided", delta = 1, method = "asymptotic", var.test = TRUE)
```

- Include scale difference
```{r, cache = TRUE}
## Asymptotic two-sided test assuming no scale difference between both samples
hl1_test(x = x * 5, y = y, alternative = "two.sided", delta = 5, method = "asymptotic", var.test = TRUE)
```

## Other packages
The tests in this package are constructed assuming homoscedasticity. Robust two-sample tests for the heteroscedastic scenario can be found in \code{R} package [WRS2](https://cran.r-project.org/web/packages/WRS2/index.html) [@MaiWil20robu].

## Outlook

## References

